---
tags: 深度學習綱要
---
# 深度學習綱要  

## [基礎知識](https://hackmd.io/knbsPpcwQDWqWGI5NJNmtQ?view)  
### [Forward & Backward](https://hackmd.io/knbsPpcwQDWqWGI5NJNmtQ?view#FB)
### [Optimizer](https://hackmd.io/gGhN0J_SQPO3rYM9PrSjMw?view)
### [Activation Function](https://hackmd.io/kzPQ1c7KRJOhF9YoBTDcaw?view)
### Normalization
### Regularization


## [學習方式](https://hackmd.io/V3lYpeuKSy6PVp5kygoEBg?view)
### 監督式學習（Supervised Learning）
### 半監督式學習（Semi-supervised Learning）
### 非監督式學習（Unsupervised Learning）
### [增強式學習（Reinforcement Learning, RL）](https://hackmd.io/yMzYrTljSTC-fY8i3Lhfgw?view)
- DQN
- RainbowDQN
- A2C
- A3C
- TD3
- DDPG
- DPPO
- C51
- IQN
- World Models
- Discrete World Models

### [Meta Learning](https://hackmd.io/qiRi4GHDTQOkfVIqEVVKFQ?view)
- memory based
- metric based
    - siamese network
    - matching network
    - prototypical network
    - relation network
- optimizer based
### 遷移學習（Transfer Learning）
### 對比學習（Contrastive Learning）
- MOCO
- SimCLR
- BYOL
- SimSiam
### Life-long learning
又稱為 Continuous Learning、Never Ending Learning、Incremental Learning


## [基底模型](https://hackmd.io/DVk6IspcR8-kG_v_sIJknw?view)
### [多層感知器（Multilayer Perceptron, MLP）](https://hackmd.io/DVk6IspcR8-kG_v_sIJknw?view#MLP)  
### [卷積神經網路（Convolutional Neural Network, CNN）](https://hackmd.io/MMgw7KGRQsC-bUME87WiCw?view)  
- Depthwise Convolution
- Pointwise Convolution
- Separable Convolution
- Flattened Convolution
- Temporal Convolution Network
- Dilated Network
- Causal Network  
### [循環神經網路（Recurrent neural network, RNN）](https://hackmd.io/sAwmoeA-RhmJckX9mJmEtA?view)
- RNN
- LSTM
- GRU 

### Multi-Head Attention

### 膠囊神經網路（Capsule Neural Network, CapsNet）  
### 圖神經網路（Graph Neural Network, GNN）  


## [進階結構](https://hackmd.io/h_nqLIMSSbS8S4Fd3EbX4A?view)
- ResNet
- DenseNet
- VoVNet
- ResNetX
- HarDNet
- Feature Pyramid Networks
- Lambda Network
- Temporal Convolution Network
- Adaptive Computation Time
- [Transformer](https://hackmd.io/vbDaWfajRyeJOobqLRD10w?view)  
    - 基本 Transformer
    - Universal Transformer
    - Transformer XL
    - Reformer
    - Performer
    - Linformer
    - Lambda Network
    - Bert
    - GPT
    - XLNet

## 可微分計算機
- Memory Augmented Neural Network (MANN)
- Neural Turing Machine (NTM)
- Differentiable Neural Computer (DNC)
- Neural GPU

## Auto Encoder
- Auto Encoder
- Variational Auto Encoder (VAE)
- Vector Quantized Variational AutoEncoder (VQVAE)
- UNet
- Nouveau Variational Auto Encoder (NVAE)

## 任務
### [生成模型](https://hackmd.io/Z-rDpZbqTz26k7D9TYXWSQ?view)
- Auto Regressive Model
- Auto Encoder
- GAN
- Flow-Based
- Diffusion Model
### [物件偵測](https://hackmd.io/M4LW1fKcRHaN5tZPhkGKcw?view)
- Anchor Based
    - RCNN
    - Fast RCNN
    - Faster RCNN
    - RetinaNet
    - EfficientDet
- Anchor Free
    - Fully Convolutional One-Stage Object Detection (FCOS)
    - CenterNet
    - RPDet
    - Cornernet
- One Stage
    - Yolo
    - Fully Convolutional One-Stage Object Detection (FCOS)
    - RetinaNet
- Two Stage
    - RCNN
    - Fast RCNN
    - Faster RCNN
    - RetinaNet
- 其他？
    - Detection with Transformer (DETR) 

### 語意分割（Semantic Segmentation）
- FCN(fully convolutional network)
### 實例分割（Instance Segmentation）
- MASK RCNN
### 音訊
- Temporal Convolution Network
- TasNet
- Conv TasNet
- Deep Complex Unet
- Transformer with Gaussian Weighted Self-Attention
- Sepformer

### Neural Radiance Fields
> https://github.com/yenchenlin/awesome-NeRF
- NeRF

### 自然語言處理
## Attention  
[參考](http://xtf615.com/2019/01/06/attention/)

## [分群方法](https://hackmd.io/yIfbZ8XdT8eR4zRgDLfiMg?view)
## [資料集](https://hackmd.io/gp_vF3XSQZOVJfiX7ELDBw?both)
## 參考文獻
## [其他資源](https://hackmd.io/SXPIpGJDTieVl2nDEAJbsA?view)
<!-- ## 基本知識
* pooling:
* stride:
* padding:
* batch norm:
* layer norm:
* instance norm:
* group norm:
* neuron
* gradient
* activation
* Regularization
* optimizer
* backpropagation
* loss function -->